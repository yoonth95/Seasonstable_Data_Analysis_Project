{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "southwest-optics",
   "metadata": {},
   "source": [
    "# 네이버 API 1000개 연관도 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-lesson",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from urllib.parse import quote\n",
    "import pandas as pd\n",
    "import requests\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from collections import Counter\n",
    "from konlpy.tag import Okt\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "import seaborn as sns\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "\n",
    "plt.rc(\"font\", family=\"Malgun Gothic\")\n",
    "#plt.rc(\"font\", family=\"AppleGothic\")\n",
    "plt.rc(\"axes\", unicode_minus=False)  #-폰트 깨지는 문제 해결\n",
    "\n",
    "\n",
    "\n",
    "def title_info(res_body):\n",
    "    ### 기본정보를 가져오는 함수\n",
    "    items = res_body['items']\n",
    "    cnt = 0\n",
    "    for item in items:\n",
    "        cnt += 1\n",
    "        news_code = cnt\n",
    "\n",
    "        title = item['title']\n",
    "        description = item['description']\n",
    "        #originallink = item['originallink']\n",
    "        title_list.append({'news_code': news_code, 'title': title, 'description': description}) #, 'originallink': originallink\n",
    "\n",
    "    return title_list\n",
    "\n",
    "##################################################################################3\n",
    "\n",
    "encText = '키워드 입력'     # 키워드 입력\n",
    "sNode = 'news'              # 검색할 사이트\n",
    "\n",
    "## cafearticle : 카페글\n",
    "## kin : 지식인\n",
    "## blog : 블로그\n",
    "## news : 뉴스\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "title_list = []\n",
    "for i in range(1, 1000, 100):\n",
    "    url = \"https://openapi.naver.com/v1/search/\" + sNode + \"?query=\" + quote(encText)  # xml 결과\n",
    "    url = url + '&start=' + str(i) + '&display=' + '100'\n",
    "    headers = {\"X-Naver-Client-Id\": \"Ck6q0uFP1OluaCauWa5j\",\n",
    "               \"X-Naver-Client-Secret\": \"ArhCyVNNv_\"}\n",
    "\n",
    "    res_json = requests.get(url, headers=headers)\n",
    "\n",
    "    if res_json.status_code == 200:\n",
    "        res_body = res_json.json()\n",
    "        #print(res_body)\n",
    "    else:\n",
    "        print(\"Error Code:\" + str(res_json.status_code))\n",
    "        sys.exit(0)\n",
    "\n",
    "\n",
    "    title_list = title_info(res_body)\n",
    "\n",
    "    \n",
    "df1 = pd.DataFrame(title_list)\n",
    "\n",
    "df1['title'] = df1['title'].str.replace('</b>', '').str.replace('<b>', '').str.replace('&quot;', '\"').str.replace('&amp;', '')\n",
    "df1['description'] = df1['description'].str.replace('</b>', '').str.replace('<b>', '').str.replace('&quot;', '\"')\n",
    "\n",
    "df1.to_csv('./data/' + encText + '_'+sNode+'.csv', index=False, encoding='utf-8')\n",
    "\n",
    "\n",
    "word_list = ['위해', '통해', '관련', '지난', '대한', '대해', '이번', '최근', '때문', '정도', '정말', '시작', '오늘', '요즘', '동안', '다른', '이후', '형태', '계속', '마치', '시간',\n",
    "              '입니다', '있는', '있다', '이런', ' 없는', '같아요', '있으니',\n",
    "                  '아니지만', '그런', '있어요', '없고', '많은', '아니다', '아니고',\n",
    "                  '아닌', '아니라', '있는데', '아니', '있습니다', '있어요', '아닙니다',\n",
    "                  '어떻게', '없는', '같은', '인해', '있어', '있지만', '없다', '있으면',\n",
    "                  '그래', '야하는', '없으니', '같아서','입니당','아니죠','어떤','그럼','이럴',\n",
    "                   '없어서','같은데','없었던','많습니다','같다','아니면','안녕하세요',\n",
    "                   '그렇지','없으면','안되서','같고','없을','있던','있답니다','아닌데',\n",
    "                   '없어도','있게','그럴','있어도','아니게','있어야','없어요','있으면서','아닌가',\n",
    "                  '있는데요','있네', '있다고', '있을까', '그러다', '있어서', '당연한', '필요한',\n",
    "                  '있을', '무사히', '있음', '있네요', '어느덧','없는데', '그렇습니다','그런거겠죠',\n",
    "                  '있지요','좋겠습니다','그렇죠','없다는','없었다','그러니','있다는',\n",
    "                  '같습니다','뿐입니다','높은','비슷한','있었지만','그렇고','푸라','없지만','없나',\n",
    "                  '있었는데','있었고','있잖아요','안되는','있었던','가능합니다','가능하다고',\n",
    "                  '있는게','있었지만','있었어요','인해서','없나','그런지','있으며','확실히','있더라구요',\n",
    "                   '있고','반갑습니다','계신','있죠','있거나','가능하다','인하여','있으','인한','안된다고',\n",
    "                   '야할지','있더라고요','있었다','그렇다','있고','없음','이러한','아님','많은','있도록',\n",
    "                  '부탁드립니다','어떠실까','어떨까','없어','같아','어떠신','필요하신', '좋은', '맛있는', '맛있게', '좋아하는',\n",
    "                  '고급스러운','고급스러', '맛있고', '좋을', '있었습니다', '중요한','더했다','아름다운가게','바쁜',\n",
    "                  '활발하게','정갈하고','유명한','깔끔하고','굉장히','쿠팡','이용', '아동',\n",
    "                    '어린이집', '아이', '정인', '인기', '없었고', '아니야', '없기', '어느새', '있다며', '다라거나', \n",
    "                   '있었음에도', '아니다라고', '없었기에','아무렇지', '그러면서', '있었을까', '없었지만', \n",
    "                   '있고요', '같다며', '있었다고', '있으니까', '다를', '뿐이었다', '생생하게', '귀여운', '친숙한',\n",
    "                  '있노라면', '완전히', '같네요', '좋지', '같다는', '같은거', '그러더니','대단히', '없습니다', '없다면'])\n",
    "\n",
    "\n",
    "\n",
    "reword = []\n",
    "for i in df1['description']:\n",
    "    okt = Okt()\n",
    "    pos_text = okt.pos(i)       \n",
    "\n",
    "\n",
    "    Noun_word = []                              \n",
    "    for word, pos in pos_text:\n",
    "        if pos == 'Noun' and len(word) >= 2 and word not in word_list:  # 두 글자 이상이고 불용어 처리한 명사만 추출\n",
    "            Noun_word.append(word)\n",
    "        \n",
    "    reword.append(Noun_word)\n",
    "    \n",
    "reword\n",
    "\n",
    "\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(reword).transform(reword)\n",
    "df_apriori = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "df_apriori\n",
    "\n",
    "\n",
    "frequent_itemsets = apriori(df_apriori, min_support=0.1, use_colnames=True)     ######## 지지도 변경\n",
    "frequent_itemsets\n",
    "\n",
    "\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n",
    "rules.sort_values(by=['lift'], ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "printable-supervision",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "suspected-lambda",
   "metadata": {},
   "source": [
    "# 네이버 API 워드클라우드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-intellectual",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from urllib.parse import quote\n",
    "import pandas as pd\n",
    "import requests\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from collections import Counter\n",
    "from konlpy.tag import Okt\n",
    "import seaborn as sns\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "\n",
    "plt.rc(\"font\", family=\"Malgun Gothic\")\n",
    "#plt.rc(\"font\", family=\"AppleGothic\")\n",
    "plt.rc(\"axes\", unicode_minus=False)  #-폰트 깨지는 문제 해결\n",
    "\n",
    "\n",
    "\n",
    "def title_info(res_body):\n",
    "    ### 기본정보를 가져오는 함수\n",
    "    items = res_body['items']\n",
    "    cnt = 0\n",
    "    for item in items:\n",
    "        cnt += 1\n",
    "        news_code = cnt\n",
    "\n",
    "        title = item['title']\n",
    "        description = item['description']\n",
    "        #originallink = item['originallink']\n",
    "        title_list.append({'news_code': news_code, 'title': title, 'description': description}) #, 'originallink': originallink\n",
    "\n",
    "    return title_list\n",
    "\n",
    "##################################################################################3\n",
    "\n",
    "encText = '키워드 입력'         # 키워드 입력\n",
    "sNode = 'cafearticle'           # 검색할 사이트\n",
    "\n",
    "## cafearticle : 카페글\n",
    "## kin : 지식인\n",
    "## blog : 블로그\n",
    "## news : 뉴스\n",
    "\n",
    "####################################################################################\n",
    "\n",
    "title_list = []\n",
    "for i in range(1, 1000, 100):\n",
    "    url = \"https://openapi.naver.com/v1/search/\"+sNode+\"?query=\" + quote(encText)  # xml 결과\n",
    "    url = url + '&start=' + str(i) + '&display=' + '100'\n",
    "    headers = {\"X-Naver-Client-Id\": \"Ck6q0uFP1OluaCauWa5j\",\n",
    "               \"X-Naver-Client-Secret\": \"ArhCyVNNv_\"}\n",
    "\n",
    "    res_json = requests.get(url, headers=headers)\n",
    "\n",
    "    if res_json.status_code == 200:\n",
    "        res_body = res_json.json()\n",
    "        #print(res_body)\n",
    "    else:\n",
    "        print(\"Error Code:\" + str(res_json.status_code))\n",
    "        sys.exit(0)\n",
    "\n",
    "\n",
    "    title_list = title_info(res_body)\n",
    "    \n",
    "df1 = pd.DataFrame(title_list)\n",
    "\n",
    "df1['title'] = df1['title'].str.replace('</b>', '').str.replace('<b>', '').str.replace('&quot;', '\"').str.replace('&amp;', '')\n",
    "df1['description'] = df1['description'].str.replace('</b>', '').str.replace('<b>', '').str.replace('&quot;', '\"')\n",
    "\n",
    "df1.to_csv('./data/' + encText + '_'+sNode+'.csv', index=False, encoding='utf-8')\n",
    "\n",
    "column_f = df1['description']\n",
    "column_f.to_csv('./data/' + encText + '_'+sNode+'.txt', index=False)         # csv 파일에서 description 열을 추출하여 txt 파일로 저장\n",
    "\n",
    "f = open('./data/' + encText + '_'+sNode+'.txt', 'r', encoding='utf-8')\n",
    "texts = f.read()\n",
    "filtered_texts = texts.replace('.', '').replace(',', '').replace(\"'\", \"\").replace('·', ' ').replace('\"', '').replace('\\n', '')\n",
    "f.close()\n",
    "\n",
    "okt = Okt()\n",
    "pos_text = okt.pos(filtered_texts)          # csv 파일을 txt 파일로 변환 후 읽어들여 형태소 분석\n",
    "#print(pos_text)\n",
    "\n",
    "Noun_word = []                              # 두 글자 이상인 명사만 추출\n",
    "for word, pos in pos_text:\n",
    "    if pos == 'Noun' and len(word) >= 2:\n",
    "        Noun_word.append(word)\n",
    "\n",
    "        \n",
    "# Adjective 형용사\n",
    "# Noun      명사\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "wcStr = ''\n",
    "for str in Noun_word:\n",
    "    wcStr = wcStr+str+'\\n'\n",
    "\n",
    "stop_words = pd.read_csv(\"https://raw.githubusercontent.com/yoonkt200/FastCampusDataset/master/korean_stopwords.txt\").values.tolist()   # RANKS NL에 제공해주는 한국어 불용어 사전\n",
    "# 참고 자료 : https://www.ranks.nl/stopwords/korean\n",
    "stop_words.extend([encText, '위해', '통해', '관련', '지난', '대한', '대해', '이번', '최근', '때문', '정도', '정말', '시작', '오늘', '요즘', '동안', '다른', '이후', '형태', '계속', '마치', '시간',\n",
    "                  '입니다', '있는', '있다', '이런', ' 없는', '같아요', '있으니',\n",
    "                  '아니지만', '그런', '있어요', '없고', '많은', '아니다', '아니고',\n",
    "                  '아닌', '아니라', '있는데', '아니', '있습니다', '있어요', '아닙니다',\n",
    "                  '어떻게', '없는', '같은', '인해', '있어', '있지만', '없다', '있으면',\n",
    "                  '그래', '야하는', '없으니', '같아서','입니당','아니죠','어떤','그럼','이럴',\n",
    "                   '없어서','같은데','없었던','많습니다','같다','아니면','안녕하세요',\n",
    "                   '그렇지','없으면','안되서','같고','없을','있던','있답니다','아닌데',\n",
    "                   '없어도','있게','그럴','있어도','아니게','있어야','없어요','있으면서','아닌가',\n",
    "                  '있는데요','있네', '있다고', '있을까', '그러다', '있어서', '당연한', '필요한',\n",
    "                  '있을', '무사히', '있음', '있네요', '어느덧','없는데', '그렇습니다','그런거겠죠',\n",
    "                  '있지요','좋겠습니다','그렇죠','없다는','없었다','그러니','있다는',\n",
    "                  '같습니다','뿐입니다','높은','비슷한','있었지만','그렇고','푸라','없지만','없나',\n",
    "                  '있었는데','있었고','있잖아요','안되는','있었던','가능합니다','가능하다고',\n",
    "                  '있는게','있었지만','있었어요','인해서','없나','그런지','있으며','확실히','있더라구요',\n",
    "                   '있고','반갑습니다','계신','있죠','있거나','가능하다','인하여','있으','인한','안된다고',\n",
    "                   '야할지','있더라고요','있었다','그렇다','있고','없음','이러한','아님','많은','있도록',\n",
    "                  '부탁드립니다','어떠실까','어떨까','없어','같아','어떠신','필요하신', '좋은', '맛있는', '맛있게', '좋아하는',\n",
    "                  '고급스러운','고급스러', '맛있고', '좋을', '있었습니다', '중요한','더했다','아름다운가게','바쁜',\n",
    "                  '활발하게','정갈하고','유명한','깔끔하고','굉장히','쿠팡','이용', '아동',\n",
    "                    '어린이집', '아이', '정인', '인기', '없었고', '아니야', '없기', '어느새', '있다며', '다라거나', \n",
    "                   '있었음에도', '아니다라고', '없었기에','아무렇지', '그러면서', '있었을까', '없었지만', \n",
    "                   '있고요', '같다며', '있었다고', '있으니까', '다를', '뿐이었다', '생생하게', '귀여운', '친숙한',\n",
    "                  '있노라면', '완전히', '같네요', '좋지', '같다는', '같은거', '그러더니','대단히', '없습니다', '없다면'])\n",
    "\n",
    "res1 = re.findall('[가-힣]+', wcStr)                  # 한글을 제외한 모든 글자 제거\n",
    "res = [w for w in res1 if w not in stop_words]       # stop_words에 있는 글자를 젲외한 나머지 글자 모으기\n",
    "\n",
    "wcStr_cnt = Counter(res)\n",
    "wcStr_tuple = wcStr_cnt.most_common()\n",
    "\n",
    "df2 = pd.DataFrame(wcStr_tuple, columns=['단어', '빈도수'])\n",
    "df2.to_csv('./data/' + encText + '_'+sNode+'_빈도수.csv', encoding='cp949', index=False)\n",
    "print(df2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############################## 워드 클라우드 생성 #####################################\n",
    "\n",
    "wc = WordCloud(font_path=\"c:/Windows/Fonts/malgun.ttf\",\n",
    "             background_color=\"white\", width=400, height=400,\n",
    "             max_words=100, max_font_size=150)\n",
    "\n",
    "#### 워드 클라우드에 텍스트를 입력해 출력\n",
    "wcFre = wc.generate_from_frequencies(wcStr_cnt)\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(wcFre, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raising-imaging",
   "metadata": {},
   "source": [
    "## 빈도수 bar 차트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-validity",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "plt.rc('font', size=20)\n",
    "ax = plt.plot()\n",
    "ax = sns.barplot(x='단어',y='빈도수', data=df2[df2['빈도수']>=50])\n",
    "\n",
    "ax.set_xlabel('단어', size = 25)  # x축 이름 설정\n",
    "ax.set_ylabel('빈도수', size = 25) # y축 이름 설정\n",
    "ax.set_title('', size = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-basket",
   "metadata": {},
   "source": [
    "## 빈도수 트리맵 차트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-wilson",
   "metadata": {},
   "outputs": [],
   "source": [
    "import squarify\n",
    "\n",
    "plt.figure(figsize=(20,15))\n",
    "plt.rc('font', size=20)\n",
    "color=['#D21E26','#1462A7','#E39120','#1462A7','gray','#E39120','#E39120','#D21E26','#D21E26',\n",
    "       '#D21E26','gray','#D21E26','#E39120']\n",
    "\n",
    "squarify.plot(sizes=df2['빈도수'], label = df2['단어'], text_kwargs={'color':'white', 'size':25}, \n",
    "              value = df2['빈도수'], color=color)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-beast",
   "metadata": {},
   "source": [
    "## 긍부정어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-mission",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "SentiWord = pd.read_json('./data/SentiWord_info.json',encoding='utf-8')\n",
    "SentiWord\n",
    "\n",
    "\n",
    "def neg_pos(word):\n",
    "    temp = pd.DataFrame()\n",
    "    global SentiWord\n",
    "    temp = SentiWord[(SentiWord['word'] == word) | (SentiWord['word_root'] == word)]\n",
    "    try:\n",
    "        word_tul = (word, temp['polarity'][temp.index[0]])\n",
    "    except IndexError:\n",
    "        word_tul = (word,0)\n",
    "    return word_tul\n",
    "\n",
    "\n",
    "res    ############################# 이건 워드클라우드 부분 검색어를 바꾸면 바뀜\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "neg_lst = []\n",
    "pos_lst = []\n",
    "unkwon_lst = []\n",
    "for df_word in tqdm(res,'긍/부정어를 나누고 있습니다.'):\n",
    "    result = neg_pos(df_word)\n",
    "    \n",
    "    if result[1] > 0:\n",
    "        pos_lst.append(result[0])\n",
    "    elif result[1] < 0:\n",
    "        neg_lst.append(result[0])\n",
    "    elif result[1] == 0:\n",
    "        unkwon_lst.append(result[0])\n",
    "        \n",
    "        \n",
    "print('긍정키워드 수:',len(pos_lst),'개')\n",
    "print('부정키워드 수:',len(neg_lst),'개')\n",
    "print('중립 or unkwon 키워드 수:',len(unkwon_lst),'개')\n",
    "\n",
    "\n",
    "from collections import Counter     # jdk에서 제공하는 라이브러리\n",
    "pos_count= dict(Counter(pos_lst).most_common())   # 내림차순 정렬\n",
    "# pos_count\n",
    "neg_count= dict(Counter(neg_lst).most_common())\n",
    "unkwon_count= dict(Counter(unkwon_lst).most_common())\n",
    "\n",
    "\n",
    "\n",
    "pos_lst = []\n",
    "neg_lst = []\n",
    "unkwon_lst = []\n",
    "\n",
    "for df_word in tqdm(res,'긍/부정어를 나누고 있습니다.'):\n",
    "    result = neg_pos(df_word)\n",
    "    if result[1] > 0:\n",
    "        pos_lst.append(result[0])\n",
    "    elif result[1] < 0:\n",
    "        neg_lst.append(result[0])\n",
    "    elif result[1] == 0:\n",
    "        unkwon_lst.append(result[0])\n",
    "        \n",
    "        \n",
    "print('긍정키워드 수:',len(pos_lst),'개')\n",
    "print('부정키워드 수:',len(neg_lst),'개')\n",
    "print('중립 or unkwon 키워드 수:',len(unkwon_lst),'개')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "\n",
    "############################ 긍정어 ######################################\n",
    "img_path = './data/good.jpg'\n",
    "cloud_mask = np.array(Image.open(img_path))\n",
    "\n",
    "def color_func(**kwargs):\n",
    "    color = '#393cc6' # https://www.w3schools.com/colors/colors_hexadecimal.asp\n",
    "    return color\n",
    "\n",
    "\n",
    "wordcloud = WordCloud(font_path=\"c:/Windows/Fonts/malgun.ttf\",\n",
    "                      background_color=\"white\", mask=cloud_mask)\n",
    "wc = wordcloud.generate_from_frequencies(pos_count)\n",
    "\n",
    "plt.figure(figsize=(10,15))\n",
    "plt.imshow(wc.recolor(color_func=color_func), interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "################################## 부정어 ################################\n",
    "img_path = './data/bad.jpg'\n",
    "cloud_mask = np.array(Image.open(img_path))\n",
    "\n",
    "def color_func(**kwargs):\n",
    "    color = '#d42b2b' # https://www.w3schools.com/colors/colors_hexadecimal.asp\n",
    "    return color\n",
    "\n",
    "wordcloud = WordCloud(font_path=\"c:/Windows/Fonts/malgun.ttf\",\n",
    "                      background_color=\"white\", mask=cloud_mask)\n",
    "wc = wordcloud.generate_from_frequencies(neg_count)\n",
    "\n",
    "plt.figure(figsize=(10,15))\n",
    "plt.imshow(wc.recolor(color_func=color_func), interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
